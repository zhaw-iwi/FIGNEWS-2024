{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-Annotator Agreement Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for this project\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk import agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Sheets API\n",
    "gc = gspread.service_account(filename='fignews-7b178eec49aa.json')\n",
    "SHEET_ID = \"1MN0ynKpEweU52-LcIRgJ_oR2-g429evqjX7T21Uz8y8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = gc.open_by_key(SHEET_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_annotations(sheet_name, column_index):\n",
    "    worksheet = spreadsheet.worksheet(sheet_name)\n",
    "    # Construct the range string to fetch from row 2 to row 201 (200 rows of data, skipping the header)\n",
    "    range_string = '{}2:{}201'.format(chr(64 + column_index), chr(64 + column_index))  # Constructs A2:A201 if column_index is 9\n",
    "    cells = worksheet.range(range_string)\n",
    "    # Return the values of the cells, not the cell objects themselves\n",
    "    values = [cell.value for cell in cells]\n",
    "    if any(value == '' for value in values):\n",
    "        raise ValueError(f\"Empty string found in sheet '{sheet_name}' in column {column_index}\")\n",
    "    return values\n",
    "\n",
    "def calculate_agreement(sheets, column_index):\n",
    "    try:\n",
    "        annotators = [fetch_annotations(sheet, column_index) for sheet in sheets]\n",
    "        taskdata = []\n",
    "        for annotator_index, ratings in enumerate(annotators):\n",
    "            taskdata += [[annotator_index, str(i), str(rating)] for i, rating in enumerate(ratings)]\n",
    "        rating_task = agreement.AnnotationTask(data=taskdata)\n",
    "        return {\n",
    "            \"Cohen's Kappa\": rating_task.kappa(),\n",
    "            \"Fleiss' Kappa\": rating_task.multi_kappa(),\n",
    "            \"Krippendorff's Alpha\": rating_task.alpha(),\n",
    "            \"Scott's Pi\": rating_task.pi()\n",
    "        }\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.433797972797157\n",
      "Fleiss' Kappa: 0.4321715382743423\n",
      "Krippendorff's Alpha: 0.4297047067460643\n",
      "Scott's Pi: 0.42899094542784894\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-2', 'IAA-3', 'IAA-4']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.6529052498080967\n",
      "Fleiss' Kappa: 0.6529052498080967\n",
      "Krippendorff's Alpha: 0.6503774602318684\n",
      "Scott's Pi: 0.649501213265031\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-2']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.32525784789384055\n",
      "Fleiss' Kappa: 0.32525784789384055\n",
      "Krippendorff's Alpha: 0.31591064955422743\n",
      "Scott's Pi: 0.3141961399039874\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-3']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.41463414634146345\n",
      "Fleiss' Kappa: 0.41463414634146345\n",
      "Krippendorff's Alpha: 0.4106448277559863\n",
      "Scott's Pi: 0.4091677471237958\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-4']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.4455079543204172\n",
      "Fleiss' Kappa: 0.4455079543204172\n",
      "Krippendorff's Alpha: 0.44493392070484583\n",
      "Scott's Pi: 0.44354277764896816\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-2', 'IAA-3']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.40773167669874794\n",
      "Fleiss' Kappa: 0.40773167669874794\n",
      "Krippendorff's Alpha: 0.4003234607027919\n",
      "Scott's Pi: 0.39882051198274876\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-2', 'IAA-4']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.35675096172037585\n",
      "Fleiss' Kappa: 0.35675096172037585\n",
      "Krippendorff's Alpha: 0.3498514329531295\n",
      "Scott's Pi: 0.34822198792293685\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-3', 'IAA-4']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.31565493913276316\n",
      "Fleiss' Kappa: 0.3144698067029619\n",
      "Krippendorff's Alpha: 0.3047482739600966\n",
      "Scott's Pi: 0.3038781216121117\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-2', 'IAA-3', 'IAA-4']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.4687108648628136\n",
      "Fleiss' Kappa: 0.4687108648628136\n",
      "Krippendorff's Alpha: 0.4631633574874584\n",
      "Scott's Pi: 0.4618179022430661\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-2']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.21977597528003093\n",
      "Fleiss' Kappa: 0.21977597528003093\n",
      "Krippendorff's Alpha: 0.2011774500475737\n",
      "Scott's Pi: 0.19917538851887087\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-3']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.35885625852094766\n",
      "Fleiss' Kappa: 0.35885625852094766\n",
      "Krippendorff's Alpha: 0.3481738803868182\n",
      "Scott's Pi: 0.34654023096422865\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-4']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.25127623369256946\n",
      "Fleiss' Kappa: 0.25127623369256946\n",
      "Krippendorff's Alpha: 0.21732152410390537\n",
      "Scott's Pi: 0.21535992391368958\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-2', 'IAA-3']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.39038506057566164\n",
      "Fleiss' Kappa: 0.39038506057566164\n",
      "Krippendorff's Alpha: 0.3879417475728155\n",
      "Scott's Pi: 0.38640776699029117\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-2', 'IAA-4']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.20492524186455582\n",
      "Fleiss' Kappa: 0.20492524186455582\n",
      "Krippendorff's Alpha: 0.1475164968140824\n",
      "Scott's Pi: 0.14537994668078427\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-3', 'IAA-4']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT vs human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_annotations_from_columns(sheet_name, columns):\n",
    "    worksheet = spreadsheet.worksheet(sheet_name)\n",
    "    annotators = []\n",
    "    for column in columns:\n",
    "        range_string = '{}2:{}201'.format(chr(64 + column), chr(64 + column))  # Constructs A2:A201 if column is 1\n",
    "        cells = worksheet.range(range_string)\n",
    "        values = [cell.value for cell in cells]\n",
    "        if any(value == '' for value in values):\n",
    "            raise ValueError(f\"Empty string found in sheet '{sheet_name}' in column {column}\")\n",
    "        annotators.append(values)\n",
    "    print(annotators)\n",
    "    return annotators\n",
    "\n",
    "def calculate_agreement_from_columns(sheet_name, columns):\n",
    "    try:\n",
    "        annotators = fetch_annotations_from_columns(sheet_name, columns)\n",
    "        taskdata = []\n",
    "        for annotator_index, ratings in enumerate(annotators):\n",
    "            taskdata += [[annotator_index, str(i), str(rating)] for i, rating in enumerate(ratings)]\n",
    "        rating_task = agreement.AnnotationTask(data=taskdata)\n",
    "        return {\n",
    "            \"Cohen's Kappa\": rating_task.kappa(),\n",
    "            \"Fleiss' Kappa\": rating_task.multi_kappa(),\n",
    "            \"Krippendorff's Alpha\": rating_task.alpha(),\n",
    "            \"Scott's Pi\": rating_task.pi()\n",
    "        }\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement Scores:\n",
      "[['Biased against others', 'Unclear', 'Biased against others', 'Biased against others', 'Unbiased', 'Unclear', 'Unbiased', 'Biased against Israel', 'Unclear', 'Unbiased', 'Unclear', 'Unbiased', 'Biased against others', 'Unclear', 'Biased against others', 'Unclear', 'Unbiased', 'Unclear', 'Biased against Israel', 'Unclear', 'Biased against others', 'Biased against others', 'Biased against others', 'Biased against Israel', 'Biased against Israel', 'Unclear', 'Not Applicable', 'Biased against others', 'Unbiased', 'Unbiased', 'Biased against others', 'Unbiased', 'Biased against others', 'Not Applicable', 'Unclear', 'Unbiased', 'Unbiased', 'Biased against others', 'Biased against Israel', 'Biased against others', 'Biased against Israel', 'Biased against others', 'Unclear', 'Unbiased', 'Unclear', 'Not Applicable', 'Unbiased', 'Biased against others', 'Unbiased', 'Biased against others', 'Biased against others', 'Unclear', 'Unclear', 'Biased against others', 'Biased against others', 'Unbiased', 'Biased against others', 'Biased against Palestine', 'Biased against others', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Biased against others', 'Unclear', 'Biased against others', 'Biased against Israel', 'Bias for', 'Biased against Palestine', 'Biased against others', 'Biased against Israel', 'Unclear', 'Biased against others', 'Unbiased', 'Biased against others', 'Biased against Israel', 'Bias for', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unclear', 'Unclear', 'Biased against Israel', 'Unclear', 'Biased against Israel', 'Biased against Israel', 'Biased against others', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unclear', 'Unclear', 'Unbiased', 'Biased against others', 'Biased against others', 'Biased against Palestine', 'Biased against others', 'Unclear', 'Biased against others', 'Unbiased', 'Biased against others', 'Bias for', 'Biased against Palestine', 'Bias for', 'Biased against others', 'Bias for', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against others', 'Unclear', 'Biased against Israel', 'Biased against Israel', 'Biased against others', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against others', 'Bias for', 'Unclear', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Biased against others', 'Unclear', 'Unbiased', 'Unbiased', 'Unclear', 'Unclear', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unclear', 'Bias for', 'Unbiased', 'Unbiased', 'Unclear', 'Bias for', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Unclear', 'Unbiased', 'Unclear', 'Biased against Israel', 'Biased against others', 'Unclear', 'Unclear', 'Unbiased', 'Unclear', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Biased against both Palestine and Israel', 'Unbiased', 'Unbiased', 'Unclear', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Unclear', 'Unclear', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Unclear', 'Biased against others', 'Unbiased', 'Unbiased', 'Unclear', 'Unbiased', 'Unclear', 'Unbiased', 'Unbiased', 'Unbiased', 'Unclear', 'Unbiased', 'Biased against others', 'Biased against others', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against others', 'Biased against others', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against others', 'Biased against others', 'Unbiased', 'Unbiased', 'Biased against Israel'], ['Biased against Palestine', 'Unbiased', 'Biased against Israel', 'Biased against Palestine', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Biased against Israel', 'Biased against Palestine', 'Unbiased', 'Biased against Palestine', 'Unbiased', 'Biased against Palestine', 'Unbiased', 'Unbiased against Palestine', 'Biased against Israel', 'Biased against Palestine', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Biased against Palestine', 'Biased against Palestine', 'Unbiased against Palestine', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Biased against Israel', 'Biased against Palestine', 'Unbiased', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Israel', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Unbiased against Israel', 'Biased against Israel', 'Biased against Palestine', 'Biased against both Palestine and Israel', 'Unclear', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Unclear', 'Biased against Palestine', 'Biased against Palestine', 'Unbiased against Israel', 'Biased against Palestine', 'Unbiased', 'Biased against Palestine', 'Biased against Palestine', 'Unbiased', 'Biased against Israel', 'Not Applicable', 'Biased against Israel', 'Biased against Palestine', 'Unbiased', 'Biased against Israel', 'Biased against Palestine', 'Unbiased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Israel', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Israel', 'Biased against Palestine', 'Biased against Israel', 'Biased against Israel', 'Biased against Israel', 'Unclear', 'Biased against Palestine', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Biased against Palestine', 'Biased against Palestine', 'Unbiased', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Not Applicable', 'Biased against others', 'Biased against Palestine', 'Unbiased', 'Biased against Palestine', 'Unbiased', 'Biased against Palestine', 'Biased against Palestine', 'Unclear', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Unbiased', 'Biased against Palestine', 'Biased against Palestine', 'Unbiased', 'Not Applicable', 'Unclear', 'Unbiased', 'Biased against Israel', 'Biased against Palestine', 'Unbiased', 'Unbiased', 'Biased against Palestine', 'Unbiased', 'Not Applicable', 'Biased against Israel', 'Biased against both Palestine and Israel', 'Unbiased', 'Unclear', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Biased against Palestine', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased against Palestine', 'Biased against Israel', 'Unclear', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Not Applicable', 'Unclear', 'Biased against Israel', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Israel', 'Unbiased', 'Biased against Palestine', 'Biased against Israel', 'Biased against Palestine', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Biased against Palestine', 'Unbiased', 'Biased against Palestine', 'Biased against Israel', 'Unbiased', 'Biased against Palestine', 'Unbiased against Palestine', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against others', 'Unclear', 'Biased against Palestine', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Biased against Palestine', 'Unclear', 'Unbiased', 'Biased against Palestine', 'Biased against Palestine', 'Biased against Palestine', 'Unbiased', 'Biased against others', 'Unbiased', 'Unbiased', 'Biased against Palestine', 'Biased against Palestine', 'Unbiased', 'Biased against Israel']]\n",
      "Cohen's Kappa: 0.19123249135556467\n",
      "Fleiss' Kappa: 0.19123249135556467\n",
      "Krippendorff's Alpha: 0.14308391434262957\n",
      "Scott's Pi: 0.14093625498007967\n"
     ]
    }
   ],
   "source": [
    "sheet_name = 'Main'  # Example sheet name\n",
    "columns = [9,13]  # Assuming annotations are in the first four columns\n",
    "\n",
    "# Calculate agreement for the annotations in the specified columns\n",
    "print(\"Agreement Scores:\")\n",
    "agreement_scores = calculate_agreement_from_columns(sheet_name, columns)\n",
    "if agreement_scores:\n",
    "    for metric, score in agreement_scores.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement Scores:\n",
      "[['Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Unclear', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Unclear', 'Unclear', 'Unclear', 'Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Unclear', 'Unclear', 'Propaganda', 'Unclear', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Unclear', 'Unclear', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Unclear', 'Not Propaganda', 'Propaganda', 'Unclear', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Unclear', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Unclear', 'Unclear', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Unclear', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Unclear', 'Propaganda', 'Propaganda', 'Unclear', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Propaganda', 'Unclear', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Unclear', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Unclear', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Not Propaganda', 'Propaganda'], ['Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Unclear', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Applicable', 'Propaganda', 'Propaganda', 'Not Applicable', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Applicable', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Applicable', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Unclear', 'Unclear', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Applicable', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Unclear', 'Not Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Unclear', 'Not Propaganda', 'Not Propaganda', 'Propaganda', 'Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Not Propaganda', 'Propaganda']]\n",
      "Cohen's Kappa: 0.35097353969046435\n",
      "Fleiss' Kappa: 0.35097353969046435\n",
      "Krippendorff's Alpha: 0.3464785183318634\n",
      "Scott's Pi: 0.34484061988156733\n"
     ]
    }
   ],
   "source": [
    "sheet_name = 'Main'  # Example sheet name\n",
    "columns = [10,12]  # Assuming annotations are in the first four columns\n",
    "\n",
    "# Calculate agreement for the annotations in the specified columns\n",
    "print(\"Agreement Scores:\")\n",
    "agreement_scores = calculate_agreement_from_columns(sheet_name, columns)\n",
    "if agreement_scores:\n",
    "    for metric, score in agreement_scores.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64 times unbiased against someone as label introduced --> Filtered out for bias\n",
    "1 \"please provide me with the text to analyse\" --> filtered out\n",
    "788 biased against palestine --> filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement Scores:\n",
      "[['Unclear', 'Biased against others', 'Unbiased', 'Unclear', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unclear', 'Unclear', 'Unclear', 'Biased against Israel', 'Unclear', 'Biased against Israel', 'Biased against Israel', 'Unclear', 'Not Applicable', 'Biased against others', 'Unbiased', 'Not Applicable', 'Unbiased', 'Not Applicable', 'Unbiased', 'Unclear', 'Biased against others', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Unclear', 'Biased against others', 'Biased against Israel', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unclear', 'Biased against Israel', 'Unclear', 'Biased against Israel', 'Biased against Israel', 'Biased against others', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against others', 'Unbiased', 'Bias for', 'Bias for', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against others', 'Unclear', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Bias for', 'Unclear', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Unclear', 'Unbiased', 'Unclear', 'Unclear', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unclear', 'Bias for', 'Unbiased', 'Unbiased', 'Bias for', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Unclear', 'Unbiased', 'Unclear', 'Biased against Israel', 'Unclear', 'Unbiased', 'Unclear', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Unclear', 'Unbiased', 'Biased against Israel', 'Unclear', 'Unbiased', 'Unclear', 'Unbiased', 'Unclear', 'Unbiased', 'Unbiased', 'Unclear', 'Unbiased', 'Biased against others', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against others', 'Unbiased', 'Biased against Israel', 'Not Applicable', 'Not Applicable', 'Unclear', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Unclear', 'Biased against Israel', 'Unbiased', 'Not Applicable', 'Unbiased', 'Unbiased', 'Not Applicable', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Not Applicable', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Unclear', 'Unbiased', 'Not Propaganda', 'Unclear', 'Not Applicable', 'Bias for', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against others', 'Biased against others', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unclear', 'Unclear', 'Not Applicable', 'Unbiased', 'Unbiased', 'Unbiased', 'Unclear', 'Biased against Israel', 'Unbiased', 'Unclear', 'Unbiased', 'Unbiased', 'Not Applicable', 'Biased against Israel', 'Unclear', 'Unbiased', 'Unbiased', 'Unbiased', 'Not Applicable', 'Unclear', 'Unbiased', 'Unbiased', 'Not Applicable', 'Unbiased', 'Unbiased', 'Not Applicable', 'Unbiased', 'Unbiased', 'Unclear', 'Not Applicable', 'Not Applicable', 'Biased against Israel', 'Not Applicable', 'Unbiased', 'Unclear'], ['Unbiased', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Biased against both Palestine and Israel', 'Unclear', 'Unclear', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Not Applicable', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Biased against Israel', 'Biased against Israel', 'Biased against Israel', 'Biased against Israel', 'Unclear', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Not Applicable', 'Biased against others', 'Unbiased', 'Unbiased', 'Unclear', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Not Applicable', 'Unclear', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Not Applicable', 'Biased against Israel', 'Biased against both Palestine and Israel', 'Unbiased', 'Unclear', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Unclear', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Not Applicable', 'Unclear', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against others', 'Unclear', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unclear', 'Unbiased', 'Unbiased', 'Biased against others', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Not Applicable', 'Unclear', 'Biased against Israel', 'Unclear', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Not Applicable', 'Unbiased', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Not Applicable', 'Biased against Israel', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Not Applicable', 'Biased against others', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Biased against Israel', 'Unbiased', 'Unbiased', 'Unbiased', 'Unbiased', 'Not Applicable', 'Biased against Israel', 'Unclear', 'Unbiased', 'Unbiased', 'Unbiased', 'Unclear', 'Unbiased', 'Unbiased', 'Unbiased', 'Not Applicable', 'Unbiased', 'Unbiased', 'Unclear', 'Unbiased', 'Unbiased', 'Not Applicable', 'Unclear', 'Biased against others', 'Unbiased', 'Biased against others', 'Unbiased', 'Unbiased']]\n",
      "Cohen's Kappa: 0.3206619932066199\n",
      "Fleiss' Kappa: 0.3206619932066199\n",
      "Krippendorff's Alpha: 0.31453322611302004\n",
      "Scott's Pi: 0.3128152642737042\n"
     ]
    }
   ],
   "source": [
    "sheet_name = 'MainClean'  # Example sheet name\n",
    "columns = [9,13]  # Assuming annotations are in the first four columns\n",
    "\n",
    "# Calculate agreement for the annotations in the specified columns\n",
    "print(\"Agreement Scores:\")\n",
    "agreement_scores = calculate_agreement_from_columns(sheet_name, columns)\n",
    "if agreement_scores:\n",
    "    for metric, score in agreement_scores.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
