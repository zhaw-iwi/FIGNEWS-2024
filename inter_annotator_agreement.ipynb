{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-Annotator Agreement Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for this project\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk import agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Sheets API\n",
    "gc = gspread.service_account(filename='fignews-7b178eec49aa.json')\n",
    "SHEET_ID = \"1MN0ynKpEweU52-LcIRgJ_oR2-g429evqjX7T21Uz8y8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = gc.open_by_key(SHEET_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_annotations(sheet_name, column_index):\n",
    "    worksheet = spreadsheet.worksheet(sheet_name)\n",
    "    # Construct the range string to fetch from row 2 to row 201 (200 rows of data, skipping the header)\n",
    "    range_string = '{}2:{}201'.format(chr(64 + column_index), chr(64 + column_index))  # Constructs A2:A201 if column_index is 9\n",
    "    cells = worksheet.range(range_string)\n",
    "    # Return the values of the cells, not the cell objects themselves\n",
    "    values = [cell.value for cell in cells]\n",
    "    if any(value == '' for value in values):\n",
    "        raise ValueError(f\"Empty string found in sheet '{sheet_name}' in column {column_index}\")\n",
    "    return values\n",
    "\n",
    "def calculate_agreement(sheets, column_index):\n",
    "    try:\n",
    "        annotators = [fetch_annotations(sheet, column_index) for sheet in sheets]\n",
    "        taskdata = []\n",
    "        for annotator_index, ratings in enumerate(annotators):\n",
    "            taskdata += [[annotator_index, str(i), str(rating)] for i, rating in enumerate(ratings)]\n",
    "        rating_task = agreement.AnnotationTask(data=taskdata)\n",
    "        return {\n",
    "            \"Cohen's Kappa\": rating_task.kappa(),\n",
    "            \"Fleiss' Kappa\": rating_task.multi_kappa(),\n",
    "            \"Krippendorff's Alpha\": rating_task.alpha(),\n",
    "            \"Scott's Pi\": rating_task.pi()\n",
    "        }\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.433797972797157\n",
      "Fleiss' Kappa: 0.4321715382743423\n",
      "Krippendorff's Alpha: 0.4297047067460643\n",
      "Scott's Pi: 0.42899094542784894\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-2', 'IAA-3', 'IAA-4']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.6529052498080967\n",
      "Fleiss' Kappa: 0.6529052498080967\n",
      "Krippendorff's Alpha: 0.6503774602318684\n",
      "Scott's Pi: 0.649501213265031\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-2']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.32525784789384055\n",
      "Fleiss' Kappa: 0.32525784789384055\n",
      "Krippendorff's Alpha: 0.31591064955422743\n",
      "Scott's Pi: 0.3141961399039874\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-3']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.41463414634146345\n",
      "Fleiss' Kappa: 0.41463414634146345\n",
      "Krippendorff's Alpha: 0.4106448277559863\n",
      "Scott's Pi: 0.4091677471237958\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-4']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.4455079543204172\n",
      "Fleiss' Kappa: 0.4455079543204172\n",
      "Krippendorff's Alpha: 0.44493392070484583\n",
      "Scott's Pi: 0.44354277764896816\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-2', 'IAA-3']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.40773167669874794\n",
      "Fleiss' Kappa: 0.40773167669874794\n",
      "Krippendorff's Alpha: 0.4003234607027919\n",
      "Scott's Pi: 0.39882051198274876\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-2', 'IAA-4']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Task Agreement Scores:\n",
      "Cohen's Kappa: 0.35675096172037585\n",
      "Fleiss' Kappa: 0.35675096172037585\n",
      "Krippendorff's Alpha: 0.3498514329531295\n",
      "Scott's Pi: 0.34822198792293685\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-3', 'IAA-4']\n",
    "column_index_1 = 9\n",
    "column_index_2 = 10\n",
    "# Calculate agreement for the first task\n",
    "print(\"Bias Task Agreement Scores:\")\n",
    "agreement_scores_1 = calculate_agreement(sheets, column_index_1)\n",
    "if agreement_scores_1:\n",
    "    for metric, score in agreement_scores_1.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.31565493913276316\n",
      "Fleiss' Kappa: 0.3144698067029619\n",
      "Krippendorff's Alpha: 0.3047482739600966\n",
      "Scott's Pi: 0.3038781216121117\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-2', 'IAA-3', 'IAA-4']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.4687108648628136\n",
      "Fleiss' Kappa: 0.4687108648628136\n",
      "Krippendorff's Alpha: 0.4631633574874584\n",
      "Scott's Pi: 0.4618179022430661\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-2']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.21977597528003093\n",
      "Fleiss' Kappa: 0.21977597528003093\n",
      "Krippendorff's Alpha: 0.2011774500475737\n",
      "Scott's Pi: 0.19917538851887087\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-3']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.35885625852094766\n",
      "Fleiss' Kappa: 0.35885625852094766\n",
      "Krippendorff's Alpha: 0.3481738803868182\n",
      "Scott's Pi: 0.34654023096422865\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-1', 'IAA-4']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.25127623369256946\n",
      "Fleiss' Kappa: 0.25127623369256946\n",
      "Krippendorff's Alpha: 0.21732152410390537\n",
      "Scott's Pi: 0.21535992391368958\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-2', 'IAA-3']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.39038506057566164\n",
      "Fleiss' Kappa: 0.39038506057566164\n",
      "Krippendorff's Alpha: 0.3879417475728155\n",
      "Scott's Pi: 0.38640776699029117\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-2', 'IAA-4']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda Task Agreement Scores:\n",
      "Cohen's Kappa: 0.20492524186455582\n",
      "Fleiss' Kappa: 0.20492524186455582\n",
      "Krippendorff's Alpha: 0.1475164968140824\n",
      "Scott's Pi: 0.14537994668078427\n"
     ]
    }
   ],
   "source": [
    "# Define the sheet names and the column index for annotations\n",
    "sheets = ['IAA-3', 'IAA-4']\n",
    "print(\"Propaganda Task Agreement Scores:\")\n",
    "agreement_scores_2 = calculate_agreement(sheets, column_index_2)\n",
    "if agreement_scores_2:\n",
    "    for metric, score in agreement_scores_2.items():\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
